{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas\n",
    "import statistics\n",
    "import threading\n",
    "import _thread\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from RankSum.analysis import brute_force as b\n",
    "from scipy import stats \n",
    "import ipykernel\n",
    "#from IPython.kernel import client\n",
    "\n",
    "from ipyparallel import *\n",
    "#import subgroup_analysis.brute_force as b\n",
    "\n",
    "\n",
    "#data load\n",
    "testing_data = pandas.read_csv(\"dataset/Data.csv\")\n",
    "training_data = pandas.read_csv(\"dataset/Training_Data.csv\")\n",
    "training_truth = pandas.read_csv(\"dataset/Training_Data_truth.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_t \n",
      "Rule            mean_in         ratio           P_val           p_val_out       people         \n",
      "(x14, 0)        -0.65292        -679.33489      0.01544         0.93473         129.00000      \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def caculation(data):\n",
    "    trt_1 = data[data['trt'] == 1]\n",
    "    trt_0 = data[data['trt'] == 0]\n",
    "    \n",
    "    medi = statistics.median(trt_1['y']) - statistics.median(trt_0['y'])\n",
    "    mean = statistics.mean(trt_1['y']) - statistics.mean(trt_0['y'])\n",
    "    peop = len(trt_1) + len(trt_0)\n",
    "    vari = statistics.variance(trt_1['y']) + statistics.variance(trt_0['y'])\n",
    "    z_stat, p_val = stats.ranksums(trt_0['y'], trt_1['y']) \n",
    "    return [medi, mean, peop, p_val]\n",
    "\n",
    "def dataset_2():\n",
    "    \n",
    "    print('dataset_{}\\n{:<15s} {:<15s} {:<15s} {:<15s} {:<15s}'.format(\"2\",\"Rule\", \"Median\", \"Mean\", \"#ofpeople\", \"P_val\"))\n",
    "    \n",
    "    rang = ((51.1,), (51.2,), (51.3,), (51.4,), (52,), (55,), (60,))\n",
    "    data = training_data[training_data['dataset'] == 2]\n",
    "\n",
    "    for a in rang:\n",
    "        data_in = data[data['x29']< a[0]]\n",
    "        data_out = data[data['x29']>= a[0]]\n",
    "\n",
    "        w(data_in, data_out, a)\n",
    "        \n",
    "    print('\\n')\n",
    "       \n",
    "    \n",
    "def dataset_3():\n",
    "    \n",
    "    print('dataset_{}\\n{:<15s} {:<15s} {:<15s} {:<15s} {:<15s}'.format(\"3\",\"Rule\", \"Median\", \"Mean\", \"#ofpeople\", \"P_val\"))\n",
    "    \n",
    "    rang = [(1, 2), (0, 2), (0, 1)]\n",
    "    data = training_data[training_data['dataset'] == 3]\n",
    "    for a in rang:\n",
    "        data_in = data[((data['x5'] == a[0]) | (data['x5'] == a[1]))]\n",
    "        data_out = data[((data['x5'] != a[0]) & (data['x5'] != a[1]))]\n",
    "        \n",
    "        w(data_in, data_out, a)\n",
    "    print('\\n')\n",
    "\n",
    "def dataset_4():\n",
    "    \n",
    "    #print('dataset_{}\\n{:<20s} {:<20s} {:<20s} {:<20s} {:<20s} {:<20s}'.format(\"4\",\"Rule\", \"Median\", \"Mean\", \"#ofpeople\", \"P_val\", \"P_val_o\"))\n",
    "    print('dataset_{} {:<20s} {:<20s} {:<20s} {:<20s} {:<20s} {:<20s}'.format(\"4\",\"Rule\", \"Mean\", \"Mean_in\",\"Mean_out\", \"people_in\", \"P_val\", \"P_val_o\"))\n",
    "    \n",
    "    rang = [(1, 2, 55), (1, 2, 57.7), (1, 2, 58), (1, 2, 60)]\n",
    "    data = training_data[training_data['dataset'] == 4]\n",
    "    for a in rang:\n",
    "        subgroup = data[((data['x4'] == a[0]) | (data['x4'] == a[1])) & (data['x22'] > a[2])]\n",
    "        notsubgroup = data[((data['x4'] != a[0]) & (data['x4'] != a[1])) | (data['x22'] <= a[2])]\n",
    "       \n",
    "        w(data_in, data_out, a)\n",
    "        print('\\n')\n",
    "   \n",
    "    \n",
    "def te():\n",
    "    \n",
    "    print('dataset_{} \\n{:<15s} {:<15s} {:<15s} {:<15s} {:<15s} {:<15s}'.format(\"t\",\"Rule\", \"mean_in\", \"ratio\", \"P_val\", \"p_val_out\", \"people\"))\n",
    "        \n",
    "\n",
    "    d = training_data[training_data['dataset'] == 2]\n",
    "    subgroup = d[(d['x29']  < 51.3)]\n",
    "    nsg= d[(d['x29'] >= 51.3 ) ]\n",
    "     \n",
    "    a = w(subgroup, nsg, ('x14', 0))\n",
    "    \n",
    "    #print(str(a) +' ' + str(b) + ' '+ str(a*b))\n",
    "    print('\\n')\n",
    "\n",
    "def w(data_in, data_out, i):\n",
    "\n",
    "    [median_in, mean_in, people_in, p_val_in] = caculation(data_in) #selection rules\n",
    "    [median_out, mean_out, people_out, p_val_out] = caculation(data_out)\n",
    "    \n",
    "    #output\n",
    "    median = median_in / median_out\n",
    "    ratio = mean_in / mean_out\n",
    "    minus = mean_in - mean_out\n",
    "    people = people_in\n",
    "    p_val = p_val_in \n",
    "    if type(i) != tuple:\n",
    "        i = tuple(i)\n",
    "    rule ='(' + str(i[0])\n",
    "    for pos in range(1, len(i)):\n",
    "        rule = rule + ', ' + str(i[pos])\n",
    "    rule = rule + ')'\n",
    "\n",
    "    print('{:<15s} {:<15.5f} {:<15.5f} {:<15.5f} {:<15.5f} {:<15.5f}'.format(rule, mean_in, ratio, p_val, p_val_out, people ))\n",
    "    \n",
    "    return ratio\n",
    "     \n",
    "#dataset_2()\n",
    "#dataset_3()\n",
    "#dataset_4()\n",
    "te()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make random \n",
    "f = open('output/rands.csv', 'w')\n",
    "f.write(\"id\")\n",
    "for i in range(1,1200):\n",
    "    f.write(\",dataset_\" + str(i))\n",
    "f.write(\"\\n\")\n",
    "for i in range(1, 241):\n",
    "    f.write(str(i))\n",
    "    for j in range(1, 1200):\n",
    "        f.write(\",\" + str(random.randint(0,1)))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "#make all ones\n",
    "f = open('output/one.csv', 'w')\n",
    "f.write(\"id\")\n",
    "for i in range(1,1200):\n",
    "    f.write(\",dataset_\" + str(i))\n",
    "f.write(\"\\n\")\n",
    "for i in range(1, 241):\n",
    "    f.write(str(i))\n",
    "    for j in range(1, 1200):\n",
    "        f.write(\",1\")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing time: 3.022982120513916\n",
      "\n",
      "thread_850: 850\n",
      "thread_720: 720\n",
      "thread_1100: 1100\n",
      "thread_960: 960\n",
      "thread_850: 851\n",
      "thread_720: 721\n",
      "thread_1100: 1101\n",
      "thread_960: 961\n",
      "thread_850: 852\n",
      "thread_720: 722\n",
      "thread_1100: 1102\n",
      "thread_960: 962\n",
      "thread_720: 723\n",
      "thread_850: 853\n",
      "thread_960: 963\n",
      "thread_1100: 1103\n",
      "thread_720: 724\n",
      "thread_850: 854\n",
      "thread_960: 964\n",
      "thread_1100: 1104\n",
      "thread_720: 725\n",
      "thread_850: 855\n",
      "thread_960: 965\n",
      "thread_1100: 1105\n",
      "thread_720: 726\n",
      "thread_850: 856\n",
      "thread_960: 966\n",
      "thread_1100: 1106\n",
      "thread_720: 727\n",
      "thread_850: 857\n",
      "thread_960: 967\n",
      "thread_1100: 1107\n",
      "thread_720: 728\n",
      "thread_850: 858\n",
      "thread_960: 968\n",
      "thread_1100: 1108\n",
      "thread_720: 729\n",
      "thread_850: 859\n",
      "thread_960: 969\n",
      "thread_1100: 1109\n",
      "thread_720: 730\n",
      "thread_850: 860\n",
      "thread_960: 970\n",
      "thread_1100: 1110\n",
      "thread_720: 731\n",
      "thread_850: 861\n",
      "thread_960: 971\n",
      "thread_1100: 1111\n",
      "thread_720: 732\n",
      "thread_850: 862\n",
      "thread_960: 972\n",
      "thread_1100: 1112\n",
      "thread_720: 733\n",
      "thread_850: 863\n",
      "thread_960: 973\n",
      "thread_1100: 1113\n",
      "thread_720: 734\n",
      "thread_850: 864\n",
      "thread_960: 974\n",
      "thread_1100: 1114\n",
      "thread_720: 735\n",
      "thread_850: 865\n",
      "thread_960: 975\n",
      "thread_1100: 1115\n",
      "thread_720: 736\n",
      "thread_850: 866\n",
      "thread_960: 976\n",
      "thread_1100: 1116\n",
      "thread_850: 867\n",
      "thread_720: 737\n",
      "thread_960: 977\n",
      "thread_1100: 1117\n",
      "thread_850: 868\n",
      "thread_720: 738\n",
      "thread_960: 978\n",
      "thread_1100: 1118\n",
      "thread_850: 869\n",
      "thread_720: 739\n",
      "thread_960: 979\n",
      "thread_1100: 1119\n",
      "thread_850: 870\n",
      "thread_720: 740\n",
      "thread_960: 980\n",
      "thread_1100: 1120\n",
      "thread_850: 871\n",
      "thread_720: 741\n",
      "thread_960: 981\n",
      "thread_1100: 1121\n",
      "thread_850: 872\n",
      "thread_720: 742\n",
      "thread_960: 982\n",
      "thread_1100: 1122\n",
      "thread_720: 743\n",
      "thread_850: 873\n",
      "thread_960: 983\n",
      "thread_1100: 1123\n",
      "thread_720: 744\n",
      "thread_850: 874\n",
      "thread_960: 984\n",
      "thread_1100: 1124\n",
      "thread_850: 875\n",
      "thread_720: 745\n",
      "thread_960: 985\n",
      "thread_1100: 1125\n",
      "thread_850: 876\n",
      "thread_720: 746\n",
      "thread_960: 986\n",
      "thread_1100: 1126\n",
      "thread_850: 877\n",
      "thread_720: 747\n",
      "thread_960: 987\n",
      "thread_1100: 1127\n",
      "thread_850: 878\n",
      "thread_720: 748\n",
      "thread_960: 988\n",
      "thread_1100: 1128\n",
      "thread_850: 879\n",
      "thread_720: 749\n",
      "thread_960: 989\n",
      "thread_1100: 1129\n",
      "thread_850: 880\n",
      "thread_720: 750\n",
      "thread_960: 990\n",
      "thread_1100: 1130\n",
      "thread_850: 881\n",
      "thread_720: 751\n",
      "thread_960: 991\n",
      "thread_1100: 1131\n",
      "thread_720: 752\n",
      "thread_850: 882\n",
      "thread_960: 992\n",
      "thread_1100: 1132\n",
      "thread_850: 883\n",
      "thread_720: 753\n",
      "thread_960: 993\n",
      "thread_1100: 1133\n",
      "thread_850: 884\n",
      "thread_720: 754\n",
      "thread_960: 994\n",
      "thread_1100: 1134\n",
      "thread_850: 885\n",
      "thread_720: 755\n",
      "thread_960: 995\n",
      "thread_1100: 1135\n",
      "thread_850: 886\n",
      "thread_720: 756\n",
      "thread_960: 996\n",
      "thread_1100: 1136\n",
      "thread_850: 887\n",
      "thread_720: 757\n",
      "thread_960: 997\n",
      "thread_1100: 1137\n",
      "thread_850: 888\n",
      "thread_720: 758\n",
      "thread_960: 998\n",
      "thread_1100: 1138\n",
      "thread_850: 889\n",
      "thread_720: 759\n",
      "thread_960: 999\n",
      "thread_1100: 1139\n",
      "thread_850: 890\n",
      "thread_720: 760\n",
      "thread_960: 1000\n",
      "thread_1100: 1140\n",
      "thread_850: 891\n",
      "thread_720: 761\n",
      "thread_960: 1001\n",
      "thread_1100: 1141\n",
      "thread_850: 892\n",
      "thread_960: 1002\n",
      "thread_720: 762\n",
      "thread_1100: 1142\n",
      "thread_850: 893\n",
      "thread_960: 1003\n",
      "thread_720: 763\n",
      "thread_1100: 1143\n",
      "thread_850: 894\n",
      "thread_960: 1004\n",
      "thread_720: 764\n",
      "thread_1100: 1144\n",
      "thread_850: 895\n",
      "thread_960: 1005\n",
      "thread_720: 765\n",
      "thread_1100: 1145\n",
      "thread_850: 896\n",
      "thread_720: 766\n",
      "thread_960: 1006\n",
      "thread_1100: 1146\n",
      "thread_850: 897\n",
      "thread_720: 767\n",
      "thread_960: 1007\n",
      "thread_1100: 1147\n",
      "thread_850: 898\n",
      "thread_720: 768\n",
      "thread_960: 1008\n",
      "thread_1100: 1148\n",
      "thread_850: 899\n",
      "thread_720: 769\n",
      "thread_960: 1009\n",
      "thread_1100: 1149\n",
      "thread_850: 900\n",
      "thread_720: 770\n",
      "thread_960: 1010\n",
      "thread_1100: 1150\n",
      "thread_850: 901\n",
      "thread_960: 1011\n",
      "thread_720: 771\n",
      "thread_1100: 1151\n",
      "thread_850: 902\n",
      "thread_960: 1012\n",
      "thread_720: 772\n",
      "thread_1100: 1152\n",
      "thread_960: 1013\n",
      "thread_720: 773\n",
      "thread_850: 903\n",
      "thread_1100: 1153\n",
      "thread_960: 1014\n",
      "thread_720: 774\n",
      "thread_850: 904\n",
      "thread_1100: 1154\n",
      "thread_960: 1015\n",
      "thread_720: 775\n",
      "thread_850: 905\n",
      "thread_1100: 1155\n",
      "thread_960: 1016\n",
      "thread_720: 776\n",
      "thread_850: 906\n",
      "thread_1100: 1156\n",
      "thread_960: 1017\n",
      "thread_720: 777\n",
      "thread_850: 907\n",
      "thread_1100: 1157\n",
      "thread_960: 1018\n",
      "thread_720: 778\n",
      "thread_850: 908\n",
      "thread_1100: 1158\n",
      "thread_960: 1019\n",
      "thread_850: 909\n",
      "thread_720: 779\n",
      "thread_1100: 1159\n",
      "thread_850: 910\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import threading\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "def Threadfun(sleeptime, start, end):\n",
    "    s = \"\"\n",
    "    for i in range(start, end): #of dataset\n",
    "        tw = testData[testData['dataset'] == i]\n",
    "        for j in range(0, 240): #of id\n",
    "            s = s + \"0\"\n",
    "            for k in range(1, 43):    #of feature\n",
    "                s = s + \" \" + str(k) + \":\" + str(tw.loc[240*(i-1) + j][k+1])\n",
    "            s = s + \"\\n\"\n",
    "        time.sleep(sleeptime)\n",
    "        print('thread_{0}: {1}'.format(start, i))\n",
    "    file = open(('dataset/svmtest_{0}'.format(start)), 'w')\n",
    "\n",
    "    file.write(s)\n",
    "    file.close \n",
    "\n",
    "def main():\n",
    "\n",
    "    # _thread.start_new_thread(Threadfun, (0.1, 1, 241))\n",
    "    # _thread.start_new_thread(Threadfun, (0.1, 600, 720))\n",
    "    _thread.start_new_thread(Threadfun, (0.1, 720, 850))\n",
    "    _thread.start_new_thread(Threadfun, (0.3, 850, 960))\n",
    "    _thread.start_new_thread(Threadfun, (0.5, 960, 1100))\n",
    "    _thread.start_new_thread(Threadfun, (0.7, 1100, 1201))\n",
    "\n",
    "#test file    \n",
    "\n",
    "testData = testing_data.copy()\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "P_VALUE_THRESHOLD = 0.05\n",
    "TRT_EFFECT_THRESHOLD = -0.6\n",
    "\n",
    "def evaluation(d):  \n",
    "    # Subgroup\n",
    "    subgroup = d[d['group'] == 1]\n",
    "    new_trt_group = subgroup[subgroup['trt'] == 1]\n",
    "    old_trt_group = subgroup[subgroup['trt'] == 0]\n",
    "    \n",
    "    if len(new_trt_group) == 0 or len(old_trt_group) == 0:\n",
    "        return 0\n",
    "    \n",
    "    trt_effect = statistics.mean(new_trt_group['y']) - statistics.mean(old_trt_group['y'])\n",
    "    number = len(new_trt_group) + len(old_trt_group)\n",
    "    z_stat, p_val = stats.ranksums(new_trt_group['y'], old_trt_group['y'])\n",
    "    \n",
    "    # Nongroup\n",
    "    nongroup = d[d['group'] == 0]\n",
    "    new_trt_group = nongroup[nongroup['trt'] == 1]\n",
    "    old_trt_group = nongroup[nongroup['trt'] == 0]\n",
    "    \n",
    "    if len(new_trt_group) == 0 or len(old_trt_group) == 0:\n",
    "        return 0\n",
    "    \n",
    "    nongroup_trt_effect = statistics.mean(new_trt_group['y']) - statistics.mean(old_trt_group['y'])\n",
    "    minus = (trt_effect-nongroup_trt_effect) if nongroup_trt_effect != 0 else 0\n",
    "    ratio = (trt_effect/nongroup_trt_effect) if nongroup_trt_effect != 0 else 0\n",
    "    z_stat, non_p_val = stats.ranksums(new_trt_group['y'], old_trt_group['y'])\n",
    "   \n",
    "    # Filter\n",
    "    if trt_effect > TRT_EFFECT_THRESHOLD: \n",
    "        return 0\n",
    "    elif p_val > P_VALUE_THRESHOLD:\n",
    "        return 0\n",
    "    \n",
    "    # Number of people restriction\n",
    "    if number < 50 or number > 150:\n",
    "        return 0\n",
    "    \n",
    "    return ratio, minus, trt_effect, nongroup_trt_effect, number, p_val, non_p_val\n",
    "\n",
    "def update_topN(score, rule, topN_score, case):\n",
    "    if score == 0:    #why??\n",
    "        return topN_score\n",
    "    \n",
    "    if case == 1:     # 1 = abs higher would be better\n",
    "        s = abs(score)\n",
    "    elif case == 2:   # 2 = close to 100 would be better\n",
    "        s = abs(score - 100)\n",
    "    else:             \n",
    "        s = score\n",
    "    topN_score.sort()  \n",
    "    \n",
    "    if len(topN_score)>=10 :\n",
    "        \n",
    "        key = []\n",
    "        if (case == 1 | case == 4):    #higher would be better\n",
    "            if s > topN_score[0][0]:    \n",
    "                key = topN_score[0]\n",
    "           \n",
    "        else:       \n",
    "            topN_score.reverse()    #lower score would be better\n",
    "            if s < topN_score[0][0]:\n",
    "                key = topN_score[0]\n",
    "        \n",
    "        if key in topN_score:\n",
    "            topN_score.remove(key)\n",
    "        else:\n",
    "            return topN_score\n",
    "        \n",
    "    topN_score.append([s, score, rule])\n",
    "    \n",
    "    return topN_score\n",
    "    \n",
    "\n",
    "# Grouping method\n",
    "def check_group(patient, rule):\n",
    "    index, values, tag = rule\n",
    "    \n",
    "    if tag == 0:\n",
    "        for v in values:\n",
    "            if patient[index] == v:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    elif tag == 1:\n",
    "        if patient[index] > values[0]:\n",
    "            return 1\n",
    "    else:\n",
    "        if patient[index] < values[0]:\n",
    "            return 1\n",
    "    return 0\n",
    "   \n",
    "\n",
    "def quartile(d):\n",
    "    Q1 = np.percentile(d, 25)\n",
    "    Q2 = np.percentile(d, 50)\n",
    "    Q3 = np.percentile(d, 75)\n",
    "    return [(Q1), (Q2), (Q3)]\n",
    "\n",
    "\n",
    "def possible_rules(d):\n",
    "    con_items = range(4, 24)\n",
    "    dis_items = range(24, 44)\n",
    "    dis_value =[[0], [1], [2], [0, 1], [0, 2], [1, 2]]\n",
    "    rules = []\n",
    "\n",
    "    for item in con_items:\n",
    "        for value in dis_value:\n",
    "            rules.append([(item), value, 0])\n",
    "            \n",
    "    for item in dis_items:\n",
    "        con_values = quartile(d['x'+str(item-3)])\n",
    "        for value in con_values:\n",
    "            rules.append([(item), [value], 1])\n",
    "            rules.append([(item), [value], 2])\n",
    "    return rules  \n",
    "\n",
    "            \n",
    "def return_key(item):\n",
    "    if item[2] == 0:\n",
    "        symbol = '='\n",
    "        key = 'x{} {} {}'.format((item[0]-3), symbol, item[1])\n",
    "            \n",
    "    else:\n",
    "        if item[2] == 1:\n",
    "            symbol = '>'\n",
    "        elif item[2] == 2:\n",
    "            symbol = '<'\n",
    "        key = 'x{} {} {:.3f}'.format((item[0]-3), symbol, item[1][0])\n",
    "    return key\n",
    "\n",
    "def rank_sum(dic, topN_score):\n",
    "    topN_score.reverse()\n",
    "    for i in range(0, len(topN_score)):\n",
    "        key = topN_score[i]\n",
    "\n",
    "        if key in dic:\n",
    "            dic[key] = dic[key] + (i+1)\n",
    "        else:\n",
    "            dic[key] = (i+1)\n",
    "    return dic\n",
    "\n",
    "def print_all(save_all):\n",
    "    print('rule \\t\\t ratio \\t   minus    trt_effec  nongroup_mean   number      p_val  non_p_val')\n",
    "    for i in save_all:\n",
    "        ratio, minus, trt_effect,nongroup_mean, number, p_val, non_p_val = i[1]\n",
    "        print('{:12s}: {:10.3f} {:10.3f}  {:10.3f} {:10.3f} {:10.3f} {:10.3f} {:10.3f}'.format(return_key(i[0]), ratio, minus, trt_effect,nongroup_mean, number, p_val, non_p_val))\n",
    "\n",
    "def write(topN_score):\n",
    "    for item in topN_score:\n",
    "        key = return_key(item[2])\n",
    "        print('{} : {:.5f}'.format(key, item[1]))\n",
    "        \n",
    "def sss(all_score, attribute, ascend):\n",
    "    count = 10\n",
    "    topN = all_score.sort_values(by = attribute, ascending = ascend).head(10)\n",
    "    for i in topN.index:\n",
    "        all_score.loc[i, 'score'] = all_score.loc[i, 'score'] + count\n",
    "        count = count-1\n",
    "    return all_score\n",
    "\n",
    "def make_upload_file(start_dataset, end_dataset, all_rules, df):\n",
    "    name = 'test'\n",
    "    output_file = 'result_{}.csv'.format(name)\n",
    "    ans = pandas.DataFrame([], columns=(['id'] + ['dataset_{}'.format(i) for i in range(1, 1201)]))\n",
    "    ans['id'] = range(1, 241)\n",
    "    if 'group' not in df:\n",
    "        df['group'] = 0\n",
    "        \n",
    "    for did in range(start_dataset, end_dataset+1):\n",
    "        \n",
    "        #initial\n",
    "        ans['dataset_{}'.format(did)] = 0\n",
    "        d = df[df['dataset'] == did].copy()\n",
    "        rule = all_rules[did-1][0]\n",
    "        print(rule)\n",
    "        if type(rule) == int:\n",
    "            continue\n",
    "\n",
    "        for i in range(1, 241):\n",
    "            patient = d[d['id'] == i].iloc[0] \n",
    "            g = check_group(patient, rule)\n",
    "            d.loc[d['id'] == i, 'group'] = g\n",
    "            \n",
    "        subgroup = d[d['group'] == 1]\n",
    "        for i in subgroup['id']:\n",
    "            ans.loc[i-1, 'dataset_{}'.format(did)] = 1\n",
    "    ans.to_csv(output_file)   \n",
    "    zf = zipfile.ZipFile('result_{}.zip'.format(name), 'w', zipfile.ZIP_DEFLATED)\n",
    "    zf.write('result_{}.csv'.format(name))\n",
    "    return ans        \n",
    "    \n",
    "def analysis(t):\n",
    "    result = []\n",
    "    df, start_dataset, end_dataset, rule_len = t\n",
    "    idx = ['rule', 'ratio', 'minus', 'trt_effect', 'nongroup_trt_effect', 'number', 'p_val', 'non_p_val', 'abs(ratio)', 'abs(number-100)', 'score']\n",
    "        \n",
    "    # Set default group number to every patients\n",
    "    if 'group' not in df:\n",
    "        df['group'] = 0\n",
    "    \n",
    "       \n",
    "    for did in range(start_dataset, end_dataset+1):\n",
    "        \n",
    "        #initial\n",
    "        all_score = pandas.DataFrame([], columns=(idx))\n",
    "        save_all = []\n",
    "        \n",
    "        print('Dataset {}'.format(did))\n",
    "        d = df[df['dataset'] == did].copy()\n",
    "        \n",
    "        for rule in possible_rules(d):\n",
    "            d = df[df['dataset'] == did].copy()\n",
    "            for i in range(1, 241):\n",
    "                patient = d[d['id'] == i].iloc[0] \n",
    "                g = check_group(patient, rule)\n",
    "                d.loc[d['id'] == i, 'group'] = g\n",
    "            \n",
    "            # Evaluate the treatment effect according each rule\n",
    "            parts = evaluation(d)\n",
    "            if type(parts) == int:\n",
    "                continue\n",
    "\n",
    "            # Caculate the socre\n",
    "            ratio, minus, trt_effect, nongroup_trt_effect, number, p_val, non_p_val = parts\n",
    "            save_all.append([rule, ratio, minus, trt_effect, nongroup_trt_effect, number, p_val, non_p_val, abs(ratio), abs(number-100), 0])\n",
    "            \n",
    "        for i in range(0, len(save_all)):\n",
    "            all_score.loc[i] = save_all[i] \n",
    "        all_score.to_csv('score/Dataset{}_score.csv'.format(did))\n",
    "\n",
    "        #topN_Ratio = sss(all_score, 'abs(ratio)', False )\n",
    "        all_score = sss(all_score, 'minus', True )\n",
    "        all_score = sss(all_score, 'trt_effect', True )\n",
    "        all_score = sss(all_score, 'nongroup_trt_effect', False )\n",
    " \n",
    "        top = all_score.sort_values(by = 'score', ascending = False).head(1)\n",
    "        if len(top['rule']) < 1:\n",
    "            result.append([0, [0], 0])\n",
    "        else:    \n",
    "            result.append(top['rule'].values[0])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1060\n",
      "Dataset 1061\n",
      "\n",
      "--- 205.28010821342468 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = (testing_data, 1060, 1061, 1)\n",
    "start_time = time.time()\n",
    "s_all = analysis(params)\n",
    "print(\"\\n--- {} seconds ---\\n\".format(time.time() - start_time))\n",
    "#跑一個dataset大概104s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1\n",
      "Dataset 151\n",
      "Dataset 301\n",
      "Dataset 451\n",
      "Dataset 601\n",
      "Dataset 751\n",
      "Dataset 901\n",
      "Dataset 1051\n",
      "Dataset 2\n",
      "Dataset 152\n",
      "Dataset 302\n",
      "Dataset 452\n",
      "Dataset 602\n",
      "Dataset 752\n",
      "Dataset 902\n",
      "Dataset 1052\n",
      "Dataset 3\n",
      "Dataset 153\n",
      "Dataset 303\n",
      "Dataset 453\n",
      "Dataset 603\n",
      "Dataset 753\n",
      "Dataset 903\n",
      "Dataset 1053\n",
      "Dataset 4\n",
      "Dataset 154\n",
      "Dataset 304\n",
      "Dataset 454\n",
      "Dataset 604\n",
      "Dataset 904\n",
      "Dataset 754\n",
      "Dataset 1054\n",
      "Dataset 5\n",
      "Dataset 155\n",
      "Dataset 305\n",
      "Dataset 455\n",
      "Dataset 605\n",
      "Dataset 905\n",
      "Dataset 755\n",
      "Dataset 1055\n",
      "Dataset 6\n",
      "Dataset 156\n",
      "Dataset 306\n",
      "Dataset 456\n",
      "Dataset 606\n",
      "Dataset 906\n",
      "Dataset 756\n",
      "Dataset 1056\n",
      "Dataset 7\n",
      "Dataset 157\n",
      "Dataset 307\n",
      "Dataset 457\n",
      "Dataset 607\n",
      "Dataset 907\n",
      "Dataset 757\n",
      "Dataset 1057\n",
      "Dataset 8\n",
      "Dataset 158\n",
      "Dataset 308\n",
      "Dataset 458\n",
      "Dataset 608\n",
      "Dataset 908\n",
      "Dataset 758\n",
      "Dataset 1058\n",
      "Dataset 9\n",
      "Dataset 159\n",
      "Dataset 309\n",
      "Dataset 459\n",
      "Dataset 609\n",
      "Dataset 909\n",
      "Dataset 759\n",
      "Dataset 1059\n",
      "Dataset 10\n",
      "Dataset 160\n",
      "Dataset 310\n",
      "Dataset 460\n",
      "Dataset 610\n",
      "Dataset 910\n",
      "Dataset 760\n",
      "Dataset 1060\n",
      "Dataset 161\n",
      "Dataset 11\n",
      "Dataset 311\n",
      "Dataset 461\n",
      "Dataset 611\n",
      "Dataset 911\n",
      "Dataset 761\n",
      "Dataset 1061\n",
      "Dataset 12\n",
      "Dataset 162\n",
      "Dataset 312\n",
      "Dataset 462\n",
      "Dataset 612\n",
      "Dataset 912\n",
      "Dataset 762\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-2-a56211851d67>\", line 238, in analysis\n    result.append(top['rule'].values[0])\nIndexError: index 0 is out of bounds for axis 0 with size 0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-211aed77d941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Calculate the execution time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parallel finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mrrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_upload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1201\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-211aed77d941>\u001b[0m in \u001b[0;36mparallel_execution\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 163\n",
      "Dataset 13\n",
      "Dataset 313\n",
      "Dataset 463\n",
      "Dataset 613\n",
      "Dataset 913\n",
      "Dataset 763\n",
      "Dataset 164\n",
      "Dataset 14\n",
      "Dataset 314\n",
      "Dataset 614\n",
      "Dataset 464\n",
      "Dataset 914\n",
      "Dataset 764\n",
      "Dataset 165\n",
      "Dataset 15\n",
      "Dataset 315\n",
      "Dataset 465\n",
      "Dataset 615\n",
      "Dataset 915\n",
      "Dataset 765\n",
      "Dataset 16\n",
      "Dataset 166\n",
      "Dataset 316\n",
      "Dataset 466\n",
      "Dataset 616\n",
      "Dataset 916\n",
      "Dataset 766\n",
      "Dataset 17\n",
      "Dataset 167\n",
      "Dataset 467\n",
      "Dataset 317\n",
      "Dataset 617\n",
      "Dataset 917\n",
      "Dataset 767\n",
      "Dataset 168\n",
      "Dataset 18\n",
      "Dataset 468\n",
      "Dataset 318\n",
      "Dataset 618\n",
      "Dataset 918\n",
      "Dataset 768\n",
      "Dataset 19\n",
      "Dataset 169\n",
      "Dataset 469\n",
      "Dataset 619\n",
      "Dataset 319\n",
      "Dataset 919\n",
      "Dataset 769\n",
      "Dataset 20\n",
      "Dataset 170\n",
      "Dataset 470\n",
      "Dataset 620\n",
      "Dataset 320\n",
      "Dataset 920\n",
      "Dataset 770\n",
      "Dataset 21\n",
      "Dataset 171\n",
      "Dataset 471\n",
      "Dataset 621\n",
      "Dataset 321\n",
      "Dataset 921\n",
      "Dataset 771\n",
      "Dataset 22\n",
      "Dataset 172\n",
      "Dataset 472\n",
      "Dataset 622\n",
      "Dataset 322\n",
      "Dataset 922\n",
      "Dataset 772\n",
      "Dataset 23\n",
      "Dataset 173\n",
      "Dataset 473\n",
      "Dataset 323\n",
      "Dataset 623\n",
      "Dataset 923\n",
      "Dataset 773\n",
      "Dataset 24\n",
      "Dataset 174\n",
      "Dataset 474\n",
      "Dataset 324\n",
      "Dataset 624\n",
      "Dataset 924\n",
      "Dataset 774\n",
      "Dataset 25\n",
      "Dataset 175\n",
      "Dataset 325\n",
      "Dataset 475\n",
      "Dataset 625\n",
      "Dataset 925\n",
      "Dataset 775\n",
      "Dataset 26\n",
      "Dataset 176\n",
      "Dataset 476\n",
      "Dataset 326\n",
      "Dataset 626\n",
      "Dataset 926\n",
      "Dataset 776\n",
      "Dataset 177\n",
      "Dataset 27\n",
      "Dataset 627\n",
      "Dataset 477\n",
      "Dataset 327\n",
      "Dataset 927\n",
      "Dataset 777\n",
      "Dataset 28\n",
      "Dataset 178\n",
      "Dataset 628\n",
      "Dataset 478\n",
      "Dataset 928\n",
      "Dataset 778\n",
      "Dataset 29\n",
      "Dataset 179\n",
      "Dataset 629\n",
      "Dataset 479\n",
      "Dataset 929\n",
      "Dataset 779\n",
      "Dataset 30\n",
      "Dataset 180\n",
      "Dataset 630\n",
      "Dataset 480\n",
      "Dataset 930\n",
      "Dataset 780\n",
      "Dataset 31\n",
      "Dataset 181\n",
      "Dataset 631\n",
      "Dataset 481\n",
      "Dataset 931\n",
      "Dataset 781\n",
      "Dataset 32\n",
      "Dataset 182\n",
      "Dataset 632\n",
      "Dataset 482\n",
      "Dataset 932\n",
      "Dataset 782\n",
      "Dataset 33\n",
      "Dataset 183\n",
      "Dataset 483\n",
      "Dataset 633\n",
      "Dataset 933\n",
      "Dataset 783\n",
      "Dataset 34\n",
      "Dataset 184\n",
      "Dataset 484\n",
      "Dataset 634\n",
      "Dataset 934\n",
      "Dataset 784\n",
      "Dataset 35\n",
      "Dataset 185\n",
      "Dataset 635\n",
      "Dataset 485\n",
      "Dataset 935\n",
      "Dataset 785\n",
      "Dataset 36\n",
      "Dataset 186\n",
      "Dataset 636\n",
      "Dataset 486\n",
      "Dataset 936\n",
      "Dataset 786\n",
      "Dataset 37\n",
      "Dataset 187\n",
      "Dataset 637\n",
      "Dataset 487\n",
      "Dataset 937\n",
      "Dataset 787\n",
      "Dataset 38\n",
      "Dataset 188\n",
      "Dataset 638\n",
      "Dataset 488\n",
      "Dataset 938\n",
      "Dataset 788\n",
      "Dataset 39\n",
      "Dataset 189\n",
      "Dataset 639\n",
      "Dataset 489\n",
      "Dataset 939\n",
      "Dataset 40\n",
      "Dataset 190\n",
      "Dataset 640\n",
      "Dataset 490\n",
      "Dataset 940\n",
      "Dataset 41\n",
      "Dataset 191\n",
      "Dataset 641\n",
      "Dataset 491\n",
      "Dataset 941\n",
      "Dataset 42\n",
      "Dataset 192\n",
      "Dataset 642\n",
      "Dataset 492\n",
      "Dataset 942\n",
      "Dataset 43\n",
      "Dataset 193\n",
      "Dataset 643\n",
      "Dataset 493\n",
      "Dataset 943\n",
      "Dataset 44\n",
      "Dataset 194\n",
      "Dataset 644\n",
      "Dataset 494\n",
      "Dataset 944\n",
      "Dataset 45\n",
      "Dataset 195\n",
      "Dataset 645\n",
      "Dataset 495\n",
      "Dataset 945\n",
      "Dataset 46\n",
      "Dataset 196\n",
      "Dataset 646\n",
      "Dataset 496\n",
      "Dataset 946\n",
      "Dataset 47\n",
      "Dataset 197\n",
      "Dataset 647\n",
      "Dataset 497\n",
      "Dataset 947\n",
      "Dataset 48\n",
      "Dataset 198\n",
      "Dataset 498\n",
      "Dataset 948\n",
      "Dataset 49\n",
      "Dataset 199\n",
      "Dataset 499\n",
      "Dataset 949\n",
      "Dataset 50\n",
      "Dataset 500\n",
      "Dataset 950\n",
      "Dataset 51\n",
      "Dataset 501\n",
      "Dataset 951\n",
      "Dataset 52\n",
      "Dataset 502\n",
      "Dataset 952\n",
      "Dataset 53\n",
      "Dataset 503\n",
      "Dataset 953\n",
      "Dataset 54\n",
      "Dataset 504\n",
      "Dataset 954\n",
      "Dataset 55\n",
      "Dataset 505\n",
      "Dataset 955\n",
      "Dataset 56\n",
      "Dataset 506\n",
      "Dataset 956\n",
      "Dataset 507\n",
      "Dataset 57\n",
      "Dataset 957\n",
      "Dataset 58\n",
      "Dataset 958\n",
      "Dataset 59\n",
      "Dataset 959\n",
      "Dataset 60\n",
      "Dataset 960\n",
      "Dataset 61\n",
      "Dataset 961\n",
      "Dataset 62\n",
      "Dataset 962\n",
      "Dataset 63\n",
      "Dataset 963\n",
      "Dataset 64\n",
      "Dataset 964\n",
      "Dataset 65\n",
      "Dataset 965\n",
      "Dataset 66\n",
      "Dataset 966\n",
      "Dataset 67\n",
      "Dataset 967\n",
      "Dataset 68\n",
      "Dataset 968\n",
      "Dataset 69\n",
      "Dataset 969\n",
      "Dataset 70\n",
      "Dataset 970\n",
      "Dataset 971\n",
      "Dataset 972\n",
      "Dataset 973\n",
      "Dataset 974\n",
      "Dataset 975\n",
      "Dataset 976\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def parallel_execution():\n",
    "    pool=Pool(8)\n",
    "    params = []\n",
    "    for i in range(8):\n",
    "        #params.append((testing_data, i+1, (i+1), 1))\n",
    "        params.append((testing_data, (i*150)+1, (i+1)*150, 1))\n",
    "\n",
    "    r = pool.map(analysis, params)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return r\n",
    "\n",
    "# Calculate the execution time\n",
    "start_time = time.time()\n",
    "results = parallel_execution()\n",
    "print('parallel finished')\n",
    "rrr = make_upload_file(1, 1201, results, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "rrr = make_upload_file(1, 2, s_all, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25, [75.717500000000001], 1], [[0, [0], 0]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Dataset 1061\n",
    "Dataset 12\n",
    "Dataset 162\n",
    "Dataset 312\n",
    "\n",
    "Dataset 462\n",
    "Dataset 612\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv = [0, [0], 0]\n",
    "vv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
